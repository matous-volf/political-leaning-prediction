{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2cde2d1f36e712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:16:47.571906Z",
     "start_time": "2025-01-27T17:16:46.567837Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.dataset_utils import get_leaning_datasets, get_politicalness_datasets, systematic_sample\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f98afe37dc67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:17:24.041120Z",
     "start_time": "2025-01-27T17:16:47.580143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measuring article_bias_prediction & article_bias_prediction\n",
      "measuring article_bias_prediction & commoncrawl_news_articles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dbcd65655d45e0afefecd21908b0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing chunks:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GET_DATASETS = lambda: get_leaning_datasets()\n",
    "# How much of CPU threads not to use in the parallelization (and so leave free).\n",
    "CPU_THREADS_RESERVED = 1\n",
    "BODY_SLICE_SIZE = 50\n",
    "BODY_ARTICLES_SAMPLE_MAX_SIZE = 10_000\n",
    "\n",
    "\n",
    "def matches(row1: pd.Series, row2: pd.Series) -> bool:\n",
    "    row1_has_title = (\"title\" in row1) and pd.notna(row1[\"title\"])\n",
    "    row1_has_body = (\"body\" in row1) and pd.notna(row1[\"body\"])\n",
    "    row2_has_title = (\"title\" in row2) and pd.notna(row2[\"title\"])\n",
    "    row2_has_body = (\"body\" in row2) and pd.notna(row2[\"body\"])\n",
    "\n",
    "    if row1_has_title and row2_has_title:\n",
    "        return row1[\"title\"] == row2[\"title\"]\n",
    "    if row1_has_body and row2_has_body:\n",
    "        return row2[\"body_slice\"] in row1[\"body\"] or row1[\"body_slice\"] in row2[\"body\"]\n",
    "    if row1_has_title and row2_has_body:\n",
    "        return row1[\"title\"] in row2[\"body\"]\n",
    "    # The only remaining option is that row 1 has a body and row 2 has a title.\n",
    "    return row2[\"title\"] in row1[\"body\"]\n",
    "\n",
    "\n",
    "def match_count(df1, df2) -> int:\n",
    "    return df1.apply(lambda row1: any(map(lambda row2: matches(row1, row2[1]), df2.iterrows())), axis=1).sum()\n",
    "\n",
    "\n",
    "def intersection_parallel(df1, df2, n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = max(1, mp.cpu_count() - CPU_THREADS_RESERVED)\n",
    "\n",
    "    chunk_size = max(len(df1) // n_processes, 1)\n",
    "    chunks = [df1.iloc[i:i + chunk_size] for i in range(0, len(df1), chunk_size)]\n",
    "\n",
    "    worker_func = partial(match_count, df2=df2)\n",
    "\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(worker_func, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=\"processing chunks\"\n",
    "        ))\n",
    "\n",
    "    return sum(results)\n",
    "\n",
    "\n",
    "datasets_count = len(list(GET_DATASETS()))\n",
    "results = [[\"\" for _ in range(datasets_count)] for _ in range(datasets_count)]\n",
    "\n",
    "for i1, dataset1 in enumerate(GET_DATASETS()):\n",
    "    dataset1 = dataset1.prepare_for_intersection_comparison(BODY_SLICE_SIZE)\n",
    "    df1 = dataset1.dataframe\n",
    "    df1_sample = systematic_sample(df1, BODY_ARTICLES_SAMPLE_MAX_SIZE)\n",
    "\n",
    "    datasets = enumerate(GET_DATASETS())\n",
    "    # Skip the first `i1` datasets to avoid measuring the intersections twice.\n",
    "    for _ in range(i1):\n",
    "        next(datasets)\n",
    "    for i2, dataset2 in datasets:\n",
    "        print(f\"measuring {dataset1.name} & {dataset2.name}\")\n",
    "\n",
    "        if i1 == i2:\n",
    "            intersection_size = len(df1)\n",
    "        else:\n",
    "            dataset2 = dataset2.prepare_for_intersection_comparison(BODY_SLICE_SIZE)\n",
    "            intersection_size = intersection_parallel(df1_sample, dataset2.dataframe) * len(df1) / len(df1_sample)\n",
    "\n",
    "        # To avoid division by zero.\n",
    "        df1_length = 1 if len(df1) == 0 else len(df1)\n",
    "        df2_length = 1 if len(dataset2.dataframe) == 0 else len(dataset2.dataframe)\n",
    "\n",
    "        intersection_size = round(intersection_size)\n",
    "        results[i1][i2] = f\"{intersection_size} ({intersection_size / df1_length * 100:.1f} %)\"\n",
    "        results[i2][i1] = f\"{intersection_size} ({intersection_size / df2_length * 100:.1f} %)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7811635bd4d8a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T17:17:43.372871150Z",
     "start_time": "2025-01-27T17:17:24.695660Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = list(map(lambda dataset: dataset.name, GET_DATASETS()))\n",
    "results_df = pd.DataFrame(results, index=dataset_names, columns=dataset_names)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
