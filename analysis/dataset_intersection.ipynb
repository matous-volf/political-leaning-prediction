{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.datasets import get_datasets\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "datasets = get_datasets()\n",
    "\n",
    "BODY_MIN_LENGTH = 500\n",
    "BODY_SLICE_SIZE = 50\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset.dataframe[\"body\"] = (dataset.dataframe[\"body\"]\n",
    "                                 .str.replace(\"<SENT_END>\", \" \")\n",
    "                                 .str.replace(r\"[^a-zA-Z]\", \"\", regex=True)\n",
    "                                 .str.lower())\n",
    "\n",
    "    if dataset.dataframe.get(\"title\") is None:\n",
    "        dataset.dataframe = dataset.dataframe[dataset.dataframe[\"body\"].str.len() >= BODY_MIN_LENGTH].copy()\n",
    "        dataset.dataframe[\"body_slice\"] = dataset.dataframe[\"body\"].map(\n",
    "            lambda body: body[\n",
    "                         math.floor(len(body) / 2 - BODY_SLICE_SIZE / 2)\n",
    "                         :math.ceil(len(body) / 2 + BODY_SLICE_SIZE / 2)\n",
    "                         ]\n",
    "        )\n",
    "    else:\n",
    "        dataset.dataframe[\"title\"] = dataset.dataframe[\"title\"].str.replace(r\"[^a-zA-Z]\", \"\", regex=True).str.lower()\n",
    "\n",
    "    print(f\"replaced {dataset.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much of CPU threads not to use in the parallelization (and so leave free)\n",
    "CPU_THREADS_RESERVED = 1\n",
    "\n",
    "def find_matches_chunk(chunk, df2, find_first_match):\n",
    "    return chunk[\"body_slice\"].apply(lambda body_slice: find_first_match(df2, body_slice)).count()\n",
    "\n",
    "\n",
    "def parallel_intersection(df1, df2, find_first_match, n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = max(1, mp.cpu_count() - CPU_THREADS_RESERVED)\n",
    "\n",
    "    chunk_size = max(len(df1) // n_processes, 1)\n",
    "    chunks = [df1.iloc[i:i + chunk_size] for i in range(0, len(df1), chunk_size)]\n",
    "\n",
    "    worker_func = partial(find_matches_chunk, df2=df2, find_first_match=find_first_match)\n",
    "\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(worker_func, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=\"processing chunks\"\n",
    "        ))\n",
    "\n",
    "    return sum(results)\n",
    "\n",
    "\n",
    "def find_first_match(haystack, needle):\n",
    "    matches = haystack[\"body\"].str.contains(needle, regex=False)\n",
    "    return None if not matches.any() else haystack[matches].iloc[0][\"body\"]\n",
    "\n",
    "\n",
    "results = [[\"\" for _ in range(len(datasets))] for _ in range(len(datasets))]\n",
    "\n",
    "for i1, dataset1 in enumerate(datasets):\n",
    "    df1 = dataset1.dataframe\n",
    "    for i2, dataset2 in map(lambda pair: (pair[0] + i1, pair[1]), enumerate(datasets[i1:])):\n",
    "        print(f\"measuring {dataset1.name} & {dataset2.name}\")\n",
    "\n",
    "        df2 = dataset2.dataframe\n",
    "\n",
    "        if dataset1 == dataset2:\n",
    "            intersection_size = len(df1)\n",
    "        elif df1.get(\"title\") is None:\n",
    "            # df1 = df1.iloc[::1000]\n",
    "            intersection_size = parallel_intersection(df1, df2, find_first_match)\n",
    "        elif df2.get(\"title\") is None:\n",
    "            # df2 = df2.iloc[::1000]\n",
    "            intersection_size = parallel_intersection(df2, df1, find_first_match)\n",
    "        else:\n",
    "            intersection_size = len(set(df1[\"title\"].str.strip())\n",
    "                                    .intersection(set(df2[\"title\"].str.strip())))\n",
    "\n",
    "        results[i1][i2] = f\"{intersection_size} ({intersection_size / len(df1) * 100:.1f} %)\"\n",
    "        results[i2][i1] = f\"{intersection_size} ({intersection_size / len(df2) * 100:.1f} %)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(map(lambda dataset: dataset.name, datasets))\n",
    "results_df = pd.DataFrame(results, index=dataset_names, columns=dataset_names)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
