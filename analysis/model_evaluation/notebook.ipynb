{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.datasets import get_datasets\n",
    "from utils.models import get_dataset_benchmark_models, get_existing_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SAMPLE_SIZE = 1000\n",
    "\n",
    "datasets = get_datasets()\n",
    "datasets_without_center = get_datasets()\n",
    "\n",
    "for dataset in datasets:\n",
    "    if len(dataset.dataframe) <= DATASET_SAMPLE_SIZE:\n",
    "        continue\n",
    "\n",
    "    dataset.dataframe = dataset.dataframe.iloc[\n",
    "                        ::len(dataset.dataframe) // DATASET_SAMPLE_SIZE\n",
    "                        ].head(DATASET_SAMPLE_SIZE)\n",
    "\n",
    "for dataset in datasets_without_center:\n",
    "    dataset.dataframe = dataset.dataframe[dataset.dataframe[\"leaning\"] != \"center\"]\n",
    "\n",
    "    if len(dataset.dataframe) <= DATASET_SAMPLE_SIZE:\n",
    "        continue\n",
    "\n",
    "    dataset.dataframe = dataset.dataframe.iloc[\n",
    "                        ::len(dataset.dataframe) // DATASET_SAMPLE_SIZE\n",
    "                        ].head(DATASET_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUNCATE_TOKENS = True\n",
    "MODELS = get_existing_models()\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for model_index, model in enumerate(MODELS):\n",
    "    print(f\"evaluating {model.name} on:\")\n",
    "    accuracy_results.append([])\n",
    "    for dataset_index in range(len(datasets)):\n",
    "        dataset = datasets[dataset_index] if model.supports_center_leaning else datasets_without_center[dataset_index]\n",
    "        print(f\"  {dataset.name}\")\n",
    "\n",
    "        predictions = []\n",
    "        for body_index, body in enumerate(tqdm(dataset.dataframe[\"body\"])):\n",
    "            try:\n",
    "                predictions.append(model.predict(body, TRUNCATE_TOKENS))\n",
    "            except RuntimeError:\n",
    "                if TRUNCATE_TOKENS:\n",
    "                    raise\n",
    "                predictions.append(None)\n",
    "\n",
    "        valid_indices = [i for i, prediction in enumerate(predictions) if prediction is not None]\n",
    "        predictions = list(map(lambda prediction: prediction.value, [predictions[i] for i in valid_indices]))\n",
    "        accuracy = accuracy_score(\n",
    "            dataset.dataframe[\"leaning\"].iloc[valid_indices].tolist(),\n",
    "            predictions\n",
    "        ) if len(predictions) > 0 else 0\n",
    "        accuracy_results[-1].append(\n",
    "            f\"{len(valid_indices) * accuracy:.0f}/{len(valid_indices)} ({np.round(accuracy * 100, 2)} %)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    accuracy_results,\n",
    "    index=list(map(lambda model: model.name, get_dataset_benchmark_models())),\n",
    "    columns=list(map(lambda dataset: dataset.name, datasets)),\n",
    ")\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
