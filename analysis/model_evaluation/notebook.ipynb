{
 "cells": [
  {
   "cell_type": "code",
   "id": "0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.datasets import get_datasets\n",
    "from utils.models import get_dataset_benchmark_models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1",
   "metadata": {},
   "source": [
    "DATASET_SAMPLE_SIZE = 1_000\n",
    "\n",
    "datasets = list(get_datasets())\n",
    "for dataset in datasets:\n",
    "    if len(dataset.dataframe) <= DATASET_SAMPLE_SIZE:\n",
    "        continue\n",
    "\n",
    "    dataset.dataframe = dataset.dataframe.iloc[\n",
    "                        ::len(dataset.dataframe) // DATASET_SAMPLE_SIZE\n",
    "                        ].head(DATASET_SAMPLE_SIZE)\n",
    "\n",
    "datasets_without_center = list(get_datasets())\n",
    "for dataset in datasets_without_center:\n",
    "    dataset.dataframe = dataset.dataframe[dataset.dataframe[\"leaning\"] != \"center\"]\n",
    "\n",
    "    if len(dataset.dataframe) <= DATASET_SAMPLE_SIZE:\n",
    "        continue\n",
    "\n",
    "    dataset.dataframe = dataset.dataframe.iloc[\n",
    "                        ::len(dataset.dataframe) // DATASET_SAMPLE_SIZE\n",
    "                        ].head(DATASET_SAMPLE_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRUNCATE_TOKENS = True\n",
    "# Lambda, so that the generator can be reused.\n",
    "GET_MODELS = lambda: get_dataset_benchmark_models()\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for model_index, model in enumerate(GET_MODELS()):\n",
    "    print(f\"evaluating {model.name} on:\")\n",
    "\n",
    "    accuracy_results.append([])\n",
    "    total_predictions_count = 0\n",
    "    total_correct_predictions_count = 0\n",
    "\n",
    "    for dataset_index in range(len(datasets)):\n",
    "        dataset = datasets[dataset_index] if model.supports_center_leaning else datasets_without_center[dataset_index]\n",
    "        print(f\"  {dataset.name}\")\n",
    "\n",
    "        predictions = []\n",
    "        for body_index, body in enumerate(tqdm(dataset.dataframe[\"body\"])):\n",
    "            try:\n",
    "                predictions.append(model.predict(body, TRUNCATE_TOKENS))\n",
    "            except RuntimeError:\n",
    "                if TRUNCATE_TOKENS:\n",
    "                    raise\n",
    "                predictions.append(None)\n",
    "\n",
    "        valid_indices = [i for i, prediction in enumerate(predictions) if prediction is not None]\n",
    "        predictions = list(map(lambda prediction: prediction.value, [predictions[i] for i in valid_indices]))\n",
    "        accuracy = accuracy_score(\n",
    "            dataset.dataframe[\"leaning\"].iloc[valid_indices].tolist(),\n",
    "            predictions\n",
    "        ) if len(predictions) > 0 else 0\n",
    "\n",
    "        predictions_count = len(valid_indices)\n",
    "        correct_predictions_count = predictions_count * accuracy\n",
    "\n",
    "        accuracy_results[-1].append(\n",
    "            f\"{correct_predictions_count:.0f} / {predictions_count} ({accuracy * 100:.0f} %)\"\n",
    "        )\n",
    "        if model.name.split(\"/\")[-1] != dataset.name:\n",
    "            total_predictions_count += predictions_count\n",
    "            total_correct_predictions_count += correct_predictions_count\n",
    "\n",
    "    average_accuracy = total_correct_predictions_count / total_predictions_count if total_predictions_count > 0 else 0\n",
    "    accuracy_results[-1].append(\n",
    "        f\"{total_correct_predictions_count:.0f} / {total_predictions_count} ({average_accuracy * 100:.0f} %)\"\n",
    "    )"
   ],
   "id": "2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3",
   "metadata": {},
   "source": [
    "results_df = pd.DataFrame(\n",
    "    accuracy_results,\n",
    "    index=list(map(lambda model: model.name, GET_MODELS())),\n",
    "    columns=list(map(lambda dataset: dataset.name, datasets)) + [\"average\"],\n",
    ")\n",
    "results_df"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
